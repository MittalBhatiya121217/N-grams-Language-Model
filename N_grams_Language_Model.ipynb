{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model\n",
        "**Instructions**:\n",
        "1. Wherever you are asked to code, insert a text block below your code block and explain what you have coded as per your own understanding.\n",
        "2. If the code is provided by us, execute it, and add below a text block and provide your explanation in your own words.\n",
        "3. Submit both the .ipynb and pdf files to canvas.\n",
        "4. **The similarity score should be less than 15%.**"
      ],
      "metadata": {
        "id": "156mVYlRucab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 (10 points)\n",
        "Explain the importance of Ngram language modeling? Provide an example of a practical application of Ngram language modeling in industry."
      ],
      "metadata": {
        "id": "N_1Sbl9avo_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer here***\n",
        "**Language model:**\n",
        "Language model is used for calculating the probablity of the sequences of the words based on conditional and joint probability.\n",
        "There are two types of the language model:\n",
        "\n",
        "1.statistical language model:\n",
        "Statistical language model is use the traditional techniques to compute the probability of the words based on the markov decision assumption adn Ngram techniques.\n",
        "2.Neural language model:\n",
        "Neural language model is use different kind of natural language models for language processing.\n",
        "\n",
        "There are different kind of gram language models are avialable that are listed below:Focusing on some of the main gram language models are:\n",
        "1.Unigram language model\n",
        "2.bigram language model\n",
        "3.Ngram language model\n",
        "Now, takling abput all the models in details we first focus on the first model,\n",
        "\n",
        "**1.unigram language model:**\n",
        "The unigram language model is basically calculating the probability of the words without condition its calculated independently.In short the unigram language model is only give the probability of the 1 word individually.\n",
        "for instance,the sentence is \"I like to eat pizza\".then, talking about the results given by the unigram model is \"I\",\"like\",\"to\",\"eat\",\"pizza\".its give each words probability indivually and not checking the conditional probability of other words of the sentences.\n",
        "**2.bigram language model**\n",
        "The bigram language model is basically calculating the probability of the two words based on condition. its consider two sequence of the words and give the probability of the occurance.For instances,the sentence is \"we are in class right now\". the results provide by bigram is like\"We are\"\",\"in class\",\"right now\",so all in all we are getting the probability of the two sequences of the words using the conditional probability.its gives atleast better result apart from the unigram.\n",
        "**3.Ngram language model**\n",
        "Talking about the Ngram language model this model is used for calculating conditional probability of the last word based on the previously occures words. For training this model they use corpus of text.Ngram language model is the sequence of number of words.\n",
        "p(w)=p(wi|wi-1)\n",
        "where, wi stands for last word of the sentece.\n",
        "wi-1 stands for last to second word of the sentence\n",
        "basically, Ngram model is usinf one or two words of the sentence and getting the probability of the language model.\n",
        "**There are some application which is used Ngram language model:**\n",
        "Ngram model is now majorly used for probability , communication theory,calculation statical natural language processing, biological sequence analysis and data compression.\n",
        "Application name of the Ngram language model used.\n",
        "Complition of sentences.\n",
        "\n",
        "For instance, text analytics,companies like Google, Amazon and Microsoft use ngram language model to develop their search engines,which is based on the ability for predict the search queries and generate matching results.Ngram is use for machine translation, where they are using to predict the translation of words and phrases based on their content.\n",
        "\n",
        "Focusing on the second example, using ngram language model is in predictive text applications like mobile phone keyboards and messaging apps. that model predict the words and phrase the typing based on the context of the previous words.\n",
        "\n",
        "Insummary, Ngram language model is an important technique in natrual language model is used to predict the next word in a sequence of words.Its has lots of practical application including search engines, machine translation and predicting text."
      ],
      "metadata": {
        "id": "tUatXR43wm4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 1: N-grams Language Model\n",
        "\n",
        "This tutorial serves as an example for creating your own Python language model. The code given Below is a  basic language model using trigrams of the Reuters corpus. A language model is a form of artificial neural network that has been trained to estimate, given some input text, the likelihood of a sequence of words or characters. The fundamental language model implies that it predicts the character that will appear after each character that has come before it.\n",
        "\n",
        "An n-gram model is a common example of a probabilistic language model. It entails estimating the likelihood of the following word in a phrase given the preceding n-1 words. For instance, a 2-gram (often referred to as a bigram) model would forecast the likelihood of the following word given the preceding word.\n",
        "\n",
        "Additionally, the tutorial below offers thorough code examples for each stage, making it simple for beginners to follow along and create their own Python language model. You should have a fundamental understanding of language modeling by the end of the lesson and be able to use this knowledge for language modeling Problems.\n",
        "\n",
        "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d"
      ],
      "metadata": {
        "id": "M7_LF7d3upxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Create a placeholder for model\n",
        "reutersmodel = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        reutersmodel[(w1, w2)][w3] += 1\n",
        "\n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in reutersmodel:\n",
        "    total_count = float(sum(reutersmodel[w1_w2].values()))\n",
        "    for w3 in reutersmodel[w1_w2]:\n",
        "        reutersmodel[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "npqZPqnUakt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e4427d8-c625-4511-db78-f592a69eca55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using dataset for generating trigram basic language model.The main aim is to do text summarization  for which is used for calculating probability of the last word based on previous words and pridict three sequences of words.\n",
        "\n",
        "import nltk - nltk stands for natural language toolkit. which is used for processing data created by human language data.\n",
        "\n",
        "nltk.download('reuters')- this is library for dataset.this dataset is the collection of the news articles related data.\n",
        "\n",
        "nltk.download('punkt')-this library is used for divide the paragraphs or long text into the list of sentences using the unsupervised algorithm for building the  model with abbreviation words, collocations and starting sentences words.\n",
        "\n",
        "from nltk.corpus import reuters - this is news corpus dataset which is used for creating basic language model using trigram language model method.\n",
        "\n",
        "from nltk import bigrams, trigrams- this are the libraries for making language model for speech recognitions. bigram is used for calculating the probability of words based on thired word with previous two words its return the two words sequences. However, trigram is calculating the probability of the words based on forth word with conditon of prevoius words it will give the result of three continues words sequences.\n",
        "\n",
        "from collections import Counter, defaultdict - This is used for collection.counter is a dictionary subclass for counting the hashable object.this is collection of elements keys and their counts are stores as dictionary values.\n",
        "collection.defaultdict is the subclass its used for specify default value for nonexistance key, if the data is not there then if its not give error insted of return the default values.\n",
        "\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))- here we are creating the placeholder for model.\n",
        "\n",
        "For loop  - we are using for loop for counting the frequency of the words occurance.\n",
        "\n",
        "calculating the probability - here we are making the for loop for calculating the words probabilities."
      ],
      "metadata": {
        "id": "Iy8Wc6kilC1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "\n",
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        "\n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold\n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in reutersmodel[tuple(text[-2:])].keys():\n",
        "      accumulator += reutersmodel[tuple(text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(word)\n",
        "          break\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        "\n",
        "print (' '.join([t for t in text if t]))"
      ],
      "metadata": {
        "id": "9DOx7m4faniI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7482161-8e57-4c4f-d4a5-f516db34f63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "today the company .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are finding the probability of the words based on the language model and finiding the sentence based on the two first starting words.\n",
        "\n",
        "import random - this is library which is used for generating the random integer and its random module.\n",
        "\n",
        "text = [\"today\", \"the\"] - here, we are pre-difining the specific starting word and search the sentence which is starting from this two words.\n",
        "\n",
        "After that we are using the constant value as a sentenced finished with false and then we are applying the threshold for random probability which is .0. additionally, we are combining all the items in to one variable using tuple now, we are selecting words based on the above threshold of the probability now we are appending the words into the text using the if condition for threshold.\n",
        "\n",
        "now we are use if condition based on text and set the constant value sentence_finished as a true.\n",
        "\n",
        "At the last , we are joining the sentence using the t for t in text if sentence start with t then return the sentence using two staring specified words.\n",
        "\n",
        "In conclude, we are usign the basic language model for finding the correct sentence based on the providing two words as a condition for fetch the sentence which is start with this two words."
      ],
      "metadata": {
        "id": "I1aXZG2KTuXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 ( 30 points)\n",
        "Choose a data set of your own choice and try building your own basic language model. You can refer to the tutorial in the provided above. Provide your understanding and explain in detail in a new text box below."
      ],
      "metadata": {
        "id": "s1tGLPc7Z19H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Create a placeholder for model\n",
        "gutenbergmodel = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance\n",
        "for sentence in gutenberg.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        gutenbergmodel[(w1, w2)][w3] += 1\n",
        "\n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in gutenbergmodel:\n",
        "    total_count = float(sum(gutenbergmodel[w1_w2].values()))\n",
        "    for w3 in gutenbergmodel[w1_w2]:\n",
        "        gutenbergmodel[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTXoNbiwAXoP",
        "outputId": "5287eb42-b7a3-4210-dbc6-028029f51868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using dataset for generating trigram basic language model.The main aim is to do text summarization for which is used for calculating probability of the last word based on previous words and pridict three sequences of words.\n",
        "\n",
        "import nltk - nltk stands for natural language toolkit. which is used for processing data created by human language data.\n",
        "\n",
        "nltk.download('gutenberg')- this is library for dataset.this datset is used for statistical analysis of the language.this dataset is the collection of the electronics books.\n",
        "\n",
        "nltk.download('punkt')-this library is used for divide the paragraphs or long text into the list of sentences using the unsupervised algorithm for building the model with abbreviation words, collocations and starting sentences words.\n",
        "\n",
        "from nltk.corpus import gutenberg - this is book corpus dataset which is used for creating basic language model using trigram language model method.\n",
        "\n",
        "from nltk import bigrams, trigrams- this are the libraries for making language model for speech recognitions. bigram is used for calculating the probability of words based on thired word with previous two words its return the two words sequences. However, trigram is calculating the probability of the words based on forth word with conditon of prevoius words it will give the result of three continues words sequences.\n",
        "\n",
        "from collections import Counter, defaultdict - This is used for collection.counter is a dictionary subclass for counting the hashable object.this is collection of elements keys and their counts are stores as dictionary values. collection.defaultdict is the subclass its used for specify default value for nonexistance key, if the data is not there then if its not give error insted of return the default values.\n",
        "\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))- here we are creating the placeholder for model.\n",
        "\n",
        "For loop - we are using for loop for counting the frequency of the words occurance.\n",
        "\n",
        "calculating the probability - here we are making the for loop for calculating the words probabilities."
      ],
      "metadata": {
        "id": "PCtq4DJAgpiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code courtesy of https://nlpforhackers.io/language-models/\n",
        "\n",
        "import random\n",
        "\n",
        "# starting words\n",
        "text = [\"today\", \"the\"]\n",
        "sentence_finished = False\n",
        "\n",
        "while not sentence_finished:\n",
        "  # select a random probability threshold\n",
        "  r = random.random()\n",
        "  accumulator = .0\n",
        "\n",
        "  for word in gutenbergmodel[tuple(text[-2:])].keys():\n",
        "      accumulator += gutenbergmodel[tuple(text[-2:])][word]\n",
        "      # select words that are above the probability threshold\n",
        "      if accumulator >= r:\n",
        "          text.append(word)\n",
        "          break\n",
        "\n",
        "  if text[-2:] == [None, None]:\n",
        "      sentence_finished = True\n",
        "\n",
        "print (' '.join([t for t in text if t]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDwcWFY4AaFz",
        "outputId": "48aed481-e83e-4dd1-84bb-a5c3e5991f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "today the conquering march of science , is but perceptibly open ' d with frantic shouts , crash of glass .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer here:**\n",
        "Here, we are finding the probability of the words based on the language model and finiding the sentence based on the two first starting words.\n",
        "\n",
        "import random - this is library which is used for generating the random integer and its random module.\n",
        "\n",
        "text = [\"today\", \"the\"] - here, we are pre-difining the specific starting word and search the sentence which is starting from this two words.\n",
        "\n",
        "After that we are using the constant value as a sentenced finished with false and then we are applying the threshold for random probability which is .0. additionally, we are combining all the items in to one variable using tuple now, we are selecting words based on the above threshold of the probability now we are appending the words into the text using the if condition for threshold.\n",
        "\n",
        "now we are use if condition based on text and set the constant value sentence_finished as a true.\n",
        "\n",
        "At the last , we are joining the sentence using the t for t in text if sentence start with t then return the sentence using two staring specified words.\n",
        "\n",
        "In conclude, we are usign the basic language model for finding the correct sentence based on the providing two words as a condition for fetch the sentence which is start with this two words."
      ],
      "metadata": {
        "id": "mAqrRwhMfy1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 (10 points)\n",
        "Explain the steps involved in building a neural language model for natural language processing (NLP)? Refer to the tutorial provided above."
      ],
      "metadata": {
        "id": "BO9wDD0Q0ebK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer here:**\n",
        "Talking about the article its majorly talking about the text summarization and creating the language model for that which is predict the next words based on the current and previous words or we can say auto complete the next predict word of the sentences.\n",
        "Focusing on the first part, they are talking about the language model as per their opinion the language model is trained to predict the probability of the sequences of the words.\n",
        "\n",
        "There are two different types of the language model:\n",
        "1. Statical language model :  This model is use for the traditional statistical techniques such as Ngram,markov model learn to probability of the words.\n",
        "2. Neural network model : this technique use different kind of neural network models.\n",
        "\n",
        "Talking about the neural network model we are basically making two different types of model character level and word level."
      ],
      "metadata": {
        "id": "C1UJV-RH1iKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-1**\n",
        "Here we are importing libraries for creating the language model using neural network language model."
      ],
      "metadata": {
        "id": "4CMYN2CywRQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras import preprocessing\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#!pip install keras_preprocessing.sequence.pad_sequences\n",
        "#import pad_sequences\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "Qq2JWX5Gwp5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here,we are importing the libraries such as,\n",
        "numpy - for processing or analytics the text.\n",
        "pandas - for performing statistical analysis of text.\n",
        "keras - keras is well known library for developing the deep learnign models.here, we are going to create sequential model from keras.models.in short here for creating neural language model we are going to use the keras model  and also we are going to use the multiple layers for the model which are lstm,dense.GRU and embedding. apart from that we are using call back library for easily stopping and model checkpoints. In short, we are importing the liabraries for generating the neural language model."
      ],
      "metadata": {
        "id": "d59SL1GtxG37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-2** Read the dataset.Here we are just reading the provided data."
      ],
      "metadata": {
        "id": "kS6TsV7J0Eqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_text = \"\"\"The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation.\n",
        "\n",
        "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.--Such has been the patient sufferance of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a candid world.\n",
        "\n",
        "He has refused his Assent to Laws, the most wholesome and necessary for the public good.\n",
        "\n",
        "He has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them.\n",
        "\n",
        "He has refused to pass other Laws for the accommodation of large districts of people, unless those people would relinquish the right of Representation in the Legislature, a right inestimable to them and formidable to tyrants only.\n",
        "\n",
        "He has called together legislative bodies at places unusual, uncomfortable, and distant from the depository of their public Records, for the sole purpose of fatiguing them into compliance with his measures.\n",
        "\n",
        "He has dissolved Representative Houses repeatedly, for opposing with manly firmness his invasions on the rights of the people.\n",
        "\n",
        "He has refused for a long time, after such dissolutions, to cause others to be elected; whereby the Legislative powers, incapable of Annihilation, have returned to the People at large for their exercise; the State remaining in the mean time exposed to all the dangers of invasion from without, and convulsions within.\n",
        "\n",
        "He has endeavoured to prevent the population of these States; for that purpose obstructing the Laws for Naturalization of Foreigners; refusing to pass others to encourage their migrations hither, and raising the conditions of new Appropriations of Lands.\n",
        "\n",
        "He has obstructed the Administration of Justice, by refusing his Assent to Laws for establishing Judiciary powers.\n",
        "\n",
        "He has made Judges dependent on his Will alone, for the tenure of their offices, and the amount and payment of their salaries.\n",
        "\n",
        "He has erected a multitude of New Offices, and sent hither swarms of Officers to harrass our people, and eat out their substance.\n",
        "\n",
        "He has kept among us, in times of peace, Standing Armies without the Consent of our legislatures.\n",
        "\n",
        "He has affected to render the Military independent of and superior to the Civil power.\n",
        "\n",
        "He has combined with others to subject us to a jurisdiction foreign to our constitution, and unacknowledged by our laws; giving his Assent to their Acts of pretended Legislation:\n",
        "\n",
        "For Quartering large bodies of armed troops among us:\n",
        "\n",
        "For protecting them, by a mock Trial, from punishment for any Murders which they should commit on the Inhabitants of these States:\n",
        "\n",
        "For cutting off our Trade with all parts of the world:\n",
        "\n",
        "For imposing Taxes on us without our Consent:\n",
        "\n",
        "For depriving us in many cases, of the benefits of Trial by Jury:\n",
        "\n",
        "For transporting us beyond Seas to be tried for pretended offences\n",
        "\n",
        "For abolishing the free System of English Laws in a neighbouring Province, establishing therein an Arbitrary government, and enlarging its Boundaries so as to render it at once an example and fit instrument for introducing the same absolute rule into these Colonies:\n",
        "\n",
        "For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally the Forms of our Governments:\n",
        "\n",
        "For suspending our own Legislatures, and declaring themselves invested with power to legislate for us in all cases whatsoever.\n",
        "\n",
        "He has abdicated Government here, by declaring us out of his Protection and waging War against us.\n",
        "\n",
        "He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of our people.\n",
        "\n",
        "He is at this time transporting large Armies of foreign Mercenaries to compleat the works of death, desolation and tyranny, already begun with circumstances of Cruelty & perfidy scarcely paralleled in the most barbarous ages, and totally unworthy the Head of a civilized nation.\n",
        "\n",
        "He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against their Country, to become the executioners of their friends and Brethren, or to fall themselves by their Hands.\n",
        "\n",
        "He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhabitants of our frontiers, the merciless Indian Savages, whose known rule of warfare, is an undistinguished destruction of all ages, sexes and conditions.\n",
        "\n",
        "In every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated Petitions have been answered only by repeated injury. A Prince whose character is thus marked by every act which may define a Tyrant, is unfit to be the ruler of a free people.\n",
        "\n",
        "Nor have We been wanting in attentions to our Brittish brethren. We have warned them from time to time of attempts by their legislature to extend an unwarrantable jurisdiction over us. We have reminded them of the circumstances of our emigration and settlement here. We have appealed to their native justice and magnanimity, and we have conjured them by the ties of our common kindred to disavow these usurpations, which, would inevitably interrupt our connections and correspondence. They too have been deaf to the voice of justice and of consanguinity. We must, therefore, acquiesce in the necessity, which denounces our Separation, and hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends.\n",
        "\n",
        "We, therefore, the Representatives of the united States of America, in General Congress, Assembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and by Authority of the good People of these Colonies, solemnly publish and declare, That these United Colonies are, and of Right ought to be Free and Independent States; that they are Absolved from all Allegiance to the British Crown, and that all political connection between them and the State of Great Britain, is and ought to be totally dissolved; and that as Free and Independent States, they have full Power to levy War, conclude Peace, contract Alliances, establish Commerce, and to do all other Acts and Things which Independent States may of right do. And for the support of this Declaration, with a firm reliance on the protection of divine Providence, we mutually pledge to each other our Lives, our Fortunes and our sacred Honor.\"\"\""
      ],
      "metadata": {
        "id": "D7tgMxsp0LTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-3** pre-process the data"
      ],
      "metadata": {
        "id": "kQZF3Rcj1TNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaner(text):\n",
        "    # lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    # remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    long_words=[]\n",
        "    # remove short word\n",
        "    for i in newString.split():\n",
        "        if len(i)>=3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# preprocess the text\n",
        "data_new = text_cleaner(data_text)"
      ],
      "metadata": {
        "id": "o4BlYLYO1uPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are importing the re which stands for regular expression. now creating the function name as a text cleaner where we passed the parameter as text.now we are converting text into lower case and now removing punctuations fro  the text using the condition after that we are removing short length words from the string.after that at the last we are joining the words and making a string. in the last we are cleaning the data using text clarner.\n",
        "\n",
        "The first step is completed the preprocess data for generating the neural language model."
      ],
      "metadata": {
        "id": "8k1RS4V82d_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-4**\n",
        "Here, now we are creating the sequence."
      ],
      "metadata": {
        "id": "4CW84Syi5_sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        # select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "        # store\n",
        "        sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences\n",
        "sequences = create_seq(data_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FScH0Y556IUR",
        "outputId": "f5133bb3-9614-44ad-96a3-978b6ff53e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 7052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are creating the sequence of the word a from the text and providing length as 30 and making list of the sequences.\n",
        "Additionally, we are creating for loop and traversing up to the length of the text and selecting the sequences of the word tokens and we are appending the tokens and finally we are printing the total sequence length of the text which is 7052. at the last we are creating object for sequences.\n",
        "\n",
        "At the last we are getting the sequence of the words list and length of the sequences for model."
      ],
      "metadata": {
        "id": "bRJtiqVq7SWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-5** Encoding the sequences"
      ],
      "metadata": {
        "id": "sRsewo2Q9O-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(set(data_new)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)"
      ],
      "metadata": {
        "id": "srqFVWyv9Z1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are encoding the character of the sequence words.\n",
        "first we sorte the new sequence of words and now mapping the words using the character and list of sequences. at the last we are encoding the sequences.\n",
        "\n",
        "At the last encoding the each words of the text into the integer number of encoding."
      ],
      "metadata": {
        "id": "0Z-gjTLjdBaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-6** Now we are creating the training and validation set"
      ],
      "metadata": {
        "id": "SEo4RdfKeYmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnNBcLdHeis0",
        "outputId": "dab8d257-7fd6-4076-9f03-dea07bff4ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (6346, 30) Val shape: (706, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, first we are importing some libraries from the train test split library. we are first checking the size of the vocabulary length using len() after that we are creating object for sequences of array. after that we are creating x and y and devide the sequences into two parts. after that we start doing the train and validation set where we we are spliting the data using the train test split.\n",
        "here we are deviding data into 90% to 10% and at the last we are printing the train shape and validation shape where the train shape is (6346,30) and validation shape is (706,30)."
      ],
      "metadata": {
        "id": "ctw_dm2Eem0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-7** we are creating a model"
      ],
      "metadata": {
        "id": "t0M5mQyRgPJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=100, verbose=2, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlIwgF_Cggl6",
        "outputId": "f97f6f14-6bd3-437e-dc49-984100fe02d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 30, 50)            1350      \n",
            "                                                                 \n",
            " gru_5 (GRU)                 (None, 150)               90900     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 27)                4077      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,327\n",
            "Trainable params: 96,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "199/199 - 15s - loss: 2.7371 - acc: 0.2164 - val_loss: 2.3978 - val_acc: 0.3286 - 15s/epoch - 76ms/step\n",
            "Epoch 2/100\n",
            "199/199 - 12s - loss: 2.2818 - acc: 0.3235 - val_loss: 2.2142 - val_acc: 0.3484 - 12s/epoch - 62ms/step\n",
            "Epoch 3/100\n",
            "199/199 - 12s - loss: 2.1493 - acc: 0.3612 - val_loss: 2.1446 - val_acc: 0.3654 - 12s/epoch - 60ms/step\n",
            "Epoch 4/100\n",
            "199/199 - 12s - loss: 2.0495 - acc: 0.3899 - val_loss: 2.1080 - val_acc: 0.3938 - 12s/epoch - 60ms/step\n",
            "Epoch 5/100\n",
            "199/199 - 12s - loss: 1.9652 - acc: 0.4114 - val_loss: 2.0212 - val_acc: 0.4065 - 12s/epoch - 60ms/step\n",
            "Epoch 6/100\n",
            "199/199 - 12s - loss: 1.8829 - acc: 0.4308 - val_loss: 2.0045 - val_acc: 0.4263 - 12s/epoch - 60ms/step\n",
            "Epoch 7/100\n",
            "199/199 - 12s - loss: 1.8079 - acc: 0.4523 - val_loss: 1.9546 - val_acc: 0.4518 - 12s/epoch - 60ms/step\n",
            "Epoch 8/100\n",
            "199/199 - 12s - loss: 1.7331 - acc: 0.4751 - val_loss: 1.9348 - val_acc: 0.4518 - 12s/epoch - 60ms/step\n",
            "Epoch 9/100\n",
            "199/199 - 12s - loss: 1.6630 - acc: 0.4954 - val_loss: 1.9241 - val_acc: 0.4632 - 12s/epoch - 59ms/step\n",
            "Epoch 10/100\n",
            "199/199 - 12s - loss: 1.5890 - acc: 0.5214 - val_loss: 1.8955 - val_acc: 0.4688 - 12s/epoch - 60ms/step\n",
            "Epoch 11/100\n",
            "199/199 - 12s - loss: 1.5291 - acc: 0.5386 - val_loss: 1.8896 - val_acc: 0.4703 - 12s/epoch - 60ms/step\n",
            "Epoch 12/100\n",
            "199/199 - 12s - loss: 1.4575 - acc: 0.5585 - val_loss: 1.8861 - val_acc: 0.4759 - 12s/epoch - 60ms/step\n",
            "Epoch 13/100\n",
            "199/199 - 12s - loss: 1.3947 - acc: 0.5763 - val_loss: 1.8811 - val_acc: 0.4887 - 12s/epoch - 60ms/step\n",
            "Epoch 14/100\n",
            "199/199 - 12s - loss: 1.3373 - acc: 0.5942 - val_loss: 1.8921 - val_acc: 0.4887 - 12s/epoch - 60ms/step\n",
            "Epoch 15/100\n",
            "199/199 - 12s - loss: 1.2861 - acc: 0.6042 - val_loss: 1.8811 - val_acc: 0.4943 - 12s/epoch - 60ms/step\n",
            "Epoch 16/100\n",
            "199/199 - 12s - loss: 1.2296 - acc: 0.6256 - val_loss: 1.9180 - val_acc: 0.4731 - 12s/epoch - 63ms/step\n",
            "Epoch 17/100\n",
            "199/199 - 12s - loss: 1.1765 - acc: 0.6347 - val_loss: 1.9039 - val_acc: 0.4887 - 12s/epoch - 60ms/step\n",
            "Epoch 18/100\n",
            "199/199 - 13s - loss: 1.1380 - acc: 0.6480 - val_loss: 1.9247 - val_acc: 0.4901 - 13s/epoch - 63ms/step\n",
            "Epoch 19/100\n",
            "199/199 - 12s - loss: 1.0834 - acc: 0.6673 - val_loss: 1.9561 - val_acc: 0.4858 - 12s/epoch - 60ms/step\n",
            "Epoch 20/100\n",
            "199/199 - 12s - loss: 1.0469 - acc: 0.6732 - val_loss: 1.9544 - val_acc: 0.5042 - 12s/epoch - 59ms/step\n",
            "Epoch 21/100\n",
            "199/199 - 12s - loss: 1.0016 - acc: 0.6921 - val_loss: 1.9655 - val_acc: 0.4972 - 12s/epoch - 61ms/step\n",
            "Epoch 22/100\n",
            "199/199 - 12s - loss: 0.9740 - acc: 0.7000 - val_loss: 1.9721 - val_acc: 0.5071 - 12s/epoch - 60ms/step\n",
            "Epoch 23/100\n",
            "199/199 - 12s - loss: 0.9397 - acc: 0.7126 - val_loss: 1.9952 - val_acc: 0.4943 - 12s/epoch - 60ms/step\n",
            "Epoch 24/100\n",
            "199/199 - 12s - loss: 0.9033 - acc: 0.7198 - val_loss: 2.0242 - val_acc: 0.4929 - 12s/epoch - 60ms/step\n",
            "Epoch 25/100\n",
            "199/199 - 12s - loss: 0.8681 - acc: 0.7272 - val_loss: 2.0593 - val_acc: 0.4901 - 12s/epoch - 60ms/step\n",
            "Epoch 26/100\n",
            "199/199 - 12s - loss: 0.8455 - acc: 0.7402 - val_loss: 2.0762 - val_acc: 0.4929 - 12s/epoch - 59ms/step\n",
            "Epoch 27/100\n",
            "199/199 - 12s - loss: 0.8151 - acc: 0.7416 - val_loss: 2.0993 - val_acc: 0.5028 - 12s/epoch - 60ms/step\n",
            "Epoch 28/100\n",
            "199/199 - 12s - loss: 0.7855 - acc: 0.7553 - val_loss: 2.1379 - val_acc: 0.4943 - 12s/epoch - 60ms/step\n",
            "Epoch 29/100\n",
            "199/199 - 12s - loss: 0.7629 - acc: 0.7589 - val_loss: 2.1549 - val_acc: 0.4873 - 12s/epoch - 60ms/step\n",
            "Epoch 30/100\n",
            "199/199 - 12s - loss: 0.7398 - acc: 0.7669 - val_loss: 2.1569 - val_acc: 0.4901 - 12s/epoch - 60ms/step\n",
            "Epoch 31/100\n",
            "199/199 - 12s - loss: 0.7337 - acc: 0.7657 - val_loss: 2.2030 - val_acc: 0.4887 - 12s/epoch - 60ms/step\n",
            "Epoch 32/100\n",
            "199/199 - 12s - loss: 0.7058 - acc: 0.7778 - val_loss: 2.1892 - val_acc: 0.4844 - 12s/epoch - 61ms/step\n",
            "Epoch 33/100\n",
            "199/199 - 12s - loss: 0.6788 - acc: 0.7847 - val_loss: 2.2277 - val_acc: 0.4972 - 12s/epoch - 60ms/step\n",
            "Epoch 34/100\n",
            "199/199 - 12s - loss: 0.6549 - acc: 0.8013 - val_loss: 2.2579 - val_acc: 0.4943 - 12s/epoch - 62ms/step\n",
            "Epoch 35/100\n",
            "199/199 - 13s - loss: 0.6536 - acc: 0.7920 - val_loss: 2.2848 - val_acc: 0.4887 - 13s/epoch - 63ms/step\n",
            "Epoch 36/100\n",
            "199/199 - 12s - loss: 0.6198 - acc: 0.8044 - val_loss: 2.3065 - val_acc: 0.4915 - 12s/epoch - 60ms/step\n",
            "Epoch 37/100\n",
            "199/199 - 12s - loss: 0.6186 - acc: 0.8030 - val_loss: 2.3069 - val_acc: 0.4887 - 12s/epoch - 60ms/step\n",
            "Epoch 38/100\n",
            "199/199 - 12s - loss: 0.5935 - acc: 0.8112 - val_loss: 2.3106 - val_acc: 0.4929 - 12s/epoch - 60ms/step\n",
            "Epoch 39/100\n",
            "199/199 - 12s - loss: 0.5852 - acc: 0.8109 - val_loss: 2.3395 - val_acc: 0.4972 - 12s/epoch - 60ms/step\n",
            "Epoch 40/100\n",
            "199/199 - 12s - loss: 0.5651 - acc: 0.8224 - val_loss: 2.3626 - val_acc: 0.4830 - 12s/epoch - 61ms/step\n",
            "Epoch 41/100\n",
            "199/199 - 12s - loss: 0.5550 - acc: 0.8245 - val_loss: 2.4099 - val_acc: 0.4802 - 12s/epoch - 60ms/step\n",
            "Epoch 42/100\n",
            "199/199 - 12s - loss: 0.5439 - acc: 0.8241 - val_loss: 2.4452 - val_acc: 0.4773 - 12s/epoch - 60ms/step\n",
            "Epoch 43/100\n",
            "199/199 - 12s - loss: 0.5413 - acc: 0.8281 - val_loss: 2.4681 - val_acc: 0.4844 - 12s/epoch - 60ms/step\n",
            "Epoch 44/100\n",
            "199/199 - 12s - loss: 0.5208 - acc: 0.8367 - val_loss: 2.4729 - val_acc: 0.4858 - 12s/epoch - 60ms/step\n",
            "Epoch 45/100\n",
            "199/199 - 12s - loss: 0.5088 - acc: 0.8405 - val_loss: 2.5007 - val_acc: 0.4802 - 12s/epoch - 60ms/step\n",
            "Epoch 46/100\n",
            "199/199 - 12s - loss: 0.4978 - acc: 0.8426 - val_loss: 2.5339 - val_acc: 0.4915 - 12s/epoch - 60ms/step\n",
            "Epoch 47/100\n",
            "199/199 - 12s - loss: 0.4945 - acc: 0.8445 - val_loss: 2.5549 - val_acc: 0.4816 - 12s/epoch - 60ms/step\n",
            "Epoch 48/100\n",
            "199/199 - 12s - loss: 0.4843 - acc: 0.8473 - val_loss: 2.5926 - val_acc: 0.4858 - 12s/epoch - 60ms/step\n",
            "Epoch 49/100\n",
            "199/199 - 13s - loss: 0.4735 - acc: 0.8483 - val_loss: 2.5928 - val_acc: 0.4844 - 13s/epoch - 63ms/step\n",
            "Epoch 50/100\n",
            "199/199 - 12s - loss: 0.4627 - acc: 0.8525 - val_loss: 2.5792 - val_acc: 0.4858 - 12s/epoch - 60ms/step\n",
            "Epoch 51/100\n",
            "199/199 - 12s - loss: 0.4484 - acc: 0.8623 - val_loss: 2.6521 - val_acc: 0.4915 - 12s/epoch - 60ms/step\n",
            "Epoch 52/100\n",
            "199/199 - 12s - loss: 0.4485 - acc: 0.8598 - val_loss: 2.6495 - val_acc: 0.4858 - 12s/epoch - 60ms/step\n",
            "Epoch 53/100\n",
            "199/199 - 12s - loss: 0.4479 - acc: 0.8547 - val_loss: 2.6702 - val_acc: 0.4788 - 12s/epoch - 60ms/step\n",
            "Epoch 54/100\n",
            "199/199 - 12s - loss: 0.4458 - acc: 0.8569 - val_loss: 2.6886 - val_acc: 0.4830 - 12s/epoch - 61ms/step\n",
            "Epoch 55/100\n",
            "199/199 - 12s - loss: 0.4361 - acc: 0.8635 - val_loss: 2.6951 - val_acc: 0.4844 - 12s/epoch - 62ms/step\n",
            "Epoch 56/100\n",
            "199/199 - 12s - loss: 0.4248 - acc: 0.8643 - val_loss: 2.7241 - val_acc: 0.4844 - 12s/epoch - 60ms/step\n",
            "Epoch 57/100\n",
            "199/199 - 12s - loss: 0.4146 - acc: 0.8651 - val_loss: 2.7402 - val_acc: 0.4915 - 12s/epoch - 60ms/step\n",
            "Epoch 58/100\n",
            "199/199 - 12s - loss: 0.4192 - acc: 0.8650 - val_loss: 2.7786 - val_acc: 0.4943 - 12s/epoch - 60ms/step\n",
            "Epoch 59/100\n",
            "199/199 - 12s - loss: 0.4181 - acc: 0.8593 - val_loss: 2.7682 - val_acc: 0.4887 - 12s/epoch - 60ms/step\n",
            "Epoch 60/100\n",
            "199/199 - 12s - loss: 0.3941 - acc: 0.8736 - val_loss: 2.8123 - val_acc: 0.4901 - 12s/epoch - 60ms/step\n",
            "Epoch 61/100\n",
            "199/199 - 12s - loss: 0.4037 - acc: 0.8706 - val_loss: 2.7980 - val_acc: 0.4802 - 12s/epoch - 60ms/step\n",
            "Epoch 62/100\n",
            "199/199 - 12s - loss: 0.3957 - acc: 0.8720 - val_loss: 2.8663 - val_acc: 0.4858 - 12s/epoch - 60ms/step\n",
            "Epoch 63/100\n",
            "199/199 - 12s - loss: 0.3950 - acc: 0.8694 - val_loss: 2.8496 - val_acc: 0.4915 - 12s/epoch - 63ms/step\n",
            "Epoch 64/100\n",
            "199/199 - 12s - loss: 0.3904 - acc: 0.8758 - val_loss: 2.8874 - val_acc: 0.4759 - 12s/epoch - 61ms/step\n",
            "Epoch 65/100\n",
            "199/199 - 12s - loss: 0.3855 - acc: 0.8728 - val_loss: 2.8660 - val_acc: 0.4731 - 12s/epoch - 60ms/step\n",
            "Epoch 66/100\n",
            "199/199 - 12s - loss: 0.3753 - acc: 0.8769 - val_loss: 2.9138 - val_acc: 0.4717 - 12s/epoch - 59ms/step\n",
            "Epoch 67/100\n",
            "199/199 - 12s - loss: 0.3716 - acc: 0.8776 - val_loss: 2.9162 - val_acc: 0.4731 - 12s/epoch - 61ms/step\n",
            "Epoch 68/100\n",
            "199/199 - 12s - loss: 0.3704 - acc: 0.8790 - val_loss: 2.9274 - val_acc: 0.4773 - 12s/epoch - 60ms/step\n",
            "Epoch 69/100\n",
            "199/199 - 12s - loss: 0.3667 - acc: 0.8801 - val_loss: 2.9383 - val_acc: 0.4646 - 12s/epoch - 60ms/step\n",
            "Epoch 70/100\n",
            "199/199 - 12s - loss: 0.3635 - acc: 0.8758 - val_loss: 2.9863 - val_acc: 0.4745 - 12s/epoch - 60ms/step\n",
            "Epoch 71/100\n",
            "199/199 - 12s - loss: 0.3504 - acc: 0.8851 - val_loss: 3.0213 - val_acc: 0.4660 - 12s/epoch - 60ms/step\n",
            "Epoch 72/100\n",
            "199/199 - 12s - loss: 0.3638 - acc: 0.8785 - val_loss: 3.0150 - val_acc: 0.4788 - 12s/epoch - 61ms/step\n",
            "Epoch 73/100\n",
            "199/199 - 12s - loss: 0.3536 - acc: 0.8847 - val_loss: 3.0198 - val_acc: 0.4745 - 12s/epoch - 60ms/step\n",
            "Epoch 74/100\n",
            "199/199 - 13s - loss: 0.3459 - acc: 0.8872 - val_loss: 3.0477 - val_acc: 0.4632 - 13s/epoch - 63ms/step\n",
            "Epoch 75/100\n",
            "199/199 - 12s - loss: 0.3389 - acc: 0.8883 - val_loss: 3.0673 - val_acc: 0.4759 - 12s/epoch - 60ms/step\n",
            "Epoch 76/100\n",
            "199/199 - 12s - loss: 0.3359 - acc: 0.8938 - val_loss: 3.1447 - val_acc: 0.4547 - 12s/epoch - 60ms/step\n",
            "Epoch 77/100\n",
            "199/199 - 12s - loss: 0.3420 - acc: 0.8892 - val_loss: 3.1342 - val_acc: 0.4688 - 12s/epoch - 61ms/step\n",
            "Epoch 78/100\n",
            "199/199 - 12s - loss: 0.3344 - acc: 0.8919 - val_loss: 3.1260 - val_acc: 0.4646 - 12s/epoch - 63ms/step\n",
            "Epoch 79/100\n",
            "199/199 - 12s - loss: 0.3392 - acc: 0.8859 - val_loss: 3.1265 - val_acc: 0.4603 - 12s/epoch - 60ms/step\n",
            "Epoch 80/100\n",
            "199/199 - 12s - loss: 0.3285 - acc: 0.8903 - val_loss: 3.1603 - val_acc: 0.4717 - 12s/epoch - 60ms/step\n",
            "Epoch 81/100\n",
            "199/199 - 12s - loss: 0.3273 - acc: 0.8917 - val_loss: 3.1588 - val_acc: 0.4717 - 12s/epoch - 60ms/step\n",
            "Epoch 82/100\n",
            "199/199 - 12s - loss: 0.3349 - acc: 0.8897 - val_loss: 3.1965 - val_acc: 0.4660 - 12s/epoch - 60ms/step\n",
            "Epoch 83/100\n",
            "199/199 - 12s - loss: 0.3190 - acc: 0.8946 - val_loss: 3.2041 - val_acc: 0.4703 - 12s/epoch - 60ms/step\n",
            "Epoch 84/100\n",
            "199/199 - 12s - loss: 0.3258 - acc: 0.8895 - val_loss: 3.2575 - val_acc: 0.4688 - 12s/epoch - 60ms/step\n",
            "Epoch 85/100\n",
            "199/199 - 12s - loss: 0.3351 - acc: 0.8865 - val_loss: 3.2633 - val_acc: 0.4618 - 12s/epoch - 60ms/step\n",
            "Epoch 86/100\n",
            "199/199 - 12s - loss: 0.3147 - acc: 0.8952 - val_loss: 3.2378 - val_acc: 0.4773 - 12s/epoch - 60ms/step\n",
            "Epoch 87/100\n",
            "199/199 - 12s - loss: 0.3192 - acc: 0.8916 - val_loss: 3.2883 - val_acc: 0.4618 - 12s/epoch - 60ms/step\n",
            "Epoch 88/100\n",
            "199/199 - 12s - loss: 0.3089 - acc: 0.8969 - val_loss: 3.3051 - val_acc: 0.4618 - 12s/epoch - 60ms/step\n",
            "Epoch 89/100\n",
            "199/199 - 12s - loss: 0.3104 - acc: 0.8927 - val_loss: 3.3089 - val_acc: 0.4731 - 12s/epoch - 60ms/step\n",
            "Epoch 90/100\n",
            "199/199 - 12s - loss: 0.3103 - acc: 0.8966 - val_loss: 3.2865 - val_acc: 0.4660 - 12s/epoch - 60ms/step\n",
            "Epoch 91/100\n",
            "199/199 - 12s - loss: 0.3016 - acc: 0.8966 - val_loss: 3.3212 - val_acc: 0.4688 - 12s/epoch - 60ms/step\n",
            "Epoch 92/100\n",
            "199/199 - 12s - loss: 0.2969 - acc: 0.9007 - val_loss: 3.3188 - val_acc: 0.4561 - 12s/epoch - 60ms/step\n",
            "Epoch 93/100\n",
            "199/199 - 13s - loss: 0.3037 - acc: 0.8922 - val_loss: 3.3772 - val_acc: 0.4731 - 13s/epoch - 67ms/step\n",
            "Epoch 94/100\n",
            "199/199 - 12s - loss: 0.2996 - acc: 0.9009 - val_loss: 3.3897 - val_acc: 0.4858 - 12s/epoch - 60ms/step\n",
            "Epoch 95/100\n",
            "199/199 - 12s - loss: 0.3047 - acc: 0.8984 - val_loss: 3.3568 - val_acc: 0.4731 - 12s/epoch - 60ms/step\n",
            "Epoch 96/100\n",
            "199/199 - 12s - loss: 0.2944 - acc: 0.8999 - val_loss: 3.3411 - val_acc: 0.4660 - 12s/epoch - 60ms/step\n",
            "Epoch 97/100\n",
            "199/199 - 12s - loss: 0.2854 - acc: 0.9043 - val_loss: 3.3599 - val_acc: 0.4759 - 12s/epoch - 61ms/step\n",
            "Epoch 98/100\n",
            "199/199 - 12s - loss: 0.2973 - acc: 0.9009 - val_loss: 3.3750 - val_acc: 0.4759 - 12s/epoch - 60ms/step\n",
            "Epoch 99/100\n",
            "199/199 - 12s - loss: 0.2918 - acc: 0.9007 - val_loss: 3.3402 - val_acc: 0.4688 - 12s/epoch - 60ms/step\n",
            "Epoch 100/100\n",
            "199/199 - 12s - loss: 0.2825 - acc: 0.9055 - val_loss: 3.4136 - val_acc: 0.4717 - 12s/epoch - 60ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb290f42d90>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are creating modek using the layers.\n",
        "first we are creating the layers of model for language model at the last we are fitting the model as well as validation data.\n",
        "\n",
        "We are using the keras to embedding layer to learn 50 dimension embedding for each character. This will help model understand complex relation between character.We are here using the GRU layer as the base model with 150 timesteps. In conclude, here we are using the final layer as a dense layer with a softmax activation for prediction.\n",
        "\n",
        "We are here providing the multiple layers such as embedding, GRU, Dense layers. after the 100 epochs of the text and we are getting the accuracy and loss as well as the evaluation score of the language model.\n",
        "\n",
        "Talking about the epochs 100 the final accuracy is 0.90 which is much acceptable while loss is 0.28 which is good ration and its shows the better performace of the language model.after that we are evaluating the model and calculating the score for the model which is validation accuracy is 0.47 which quite acceptable and better while the validation loss is 3.41 which is higher than what we got while generating model. so, all in all the neural language model is better in performace and accurate to predict the next word in the word sequences."
      ],
      "metadata": {
        "id": "wQob5ZpEgf0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-8** Now model is created now we are input the text into the model and creating new text."
      ],
      "metadata": {
        "id": "eI2utCYPlKmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\t\t# encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in in_text]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t#predict character\n",
        "\t\tyhat=model.predict(encoded)\n",
        "\t\tyhat=np.argmax(yhat,axis=1)\n",
        "\t\t#yhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# reverse map integer to character\n",
        "\t\tout_char = ''\n",
        "\t\tfor char, index in mapping.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += char\n",
        "\treturn in_text\n",
        "\n",
        "inp='large armies for'\n",
        "print(len(inp))\n",
        "print(generate_seq(model, mapping, 30, inp.lower(),15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXJoEntklaHE",
        "outputId": "11c22a21-2371-452e-86bf-0ecab54bb3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "large armies foreign our consti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are using the created neural language model and now we are inputing the text into the model and generating new text of sequences.\n",
        "\n",
        "Here we are inserting the text into the model to check the model working condition. at the last we provide some words and we need to get the sequences of the words after that.\n",
        "\n",
        "Here we input the text into neural language model and check whether its working perfectly or not and based on the results its working perfectly and predicting the next word of the sequence is quite accurate. we also calculate the length of the input to get the better result or we can say better prediction of next word of the sequences from the model.\n",
        "\n",
        "In conclude, its proven that neural language model is more accurate and better performer for language model and as well as for achiving the goal for machine translation neural language model is accurate."
      ],
      "metadata": {
        "id": "rcpJ-I50tak6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4 (10 points)\n",
        "What are the limitations of N-gram language modeling in natural language processing? Provide an example of a situation where N-gram language modeling may not be effective."
      ],
      "metadata": {
        "id": "3qAUvtcJSotH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer here:**\n",
        "N-gram language model is the most popular technique in neural language processing that predict the probability of the next word based on the condition n-1 word.\n",
        "\n",
        "Talking about the limitation of the N-gram language modeling in natural language processing.\n",
        "\n",
        "Sparsity : N-gram model is suffering from the sparsity issues when dealing with rare or unseen n-gram. when the n-grams increases the probabilities of encountering unseen n-gram increase and finally result the inaccurate predictions.\n",
        "\n",
        "Ambiguity :  n-gram doesn't handle ambiguity effectively.In sequence of words with same probability where the multiple words follows the sequence of words nut model may face the issue to choose the correct sequence one.\n",
        "\n",
        "limited context: n-hram models only consider a limited number of previous words in the sentences, its typically range between 1 word to 5 words, that means may be that don't able to capture the long dependent words between words and result the inaccurate pridictions.\n",
        "\n",
        "For instance of if there is a situation where N-gram language model may not able to effective in machine translation. machine translation sometimes meaning of sentence can change based on the order of the words.n-gram model may be not able to work because of the its limited context and ambiguity.\n",
        "\n",
        "Talking about the example, if the sentence is \" I am happy to see you\" or another sentence is \"I am glad that you can see\" So, the correct translation based on the context and the meaning of intention, which may be N-gram model may not able to identify correctly.\n",
        "\n",
        "In conclude, N-gram model is good with the short sentences where we can define the words and predict the next word based on the previous words. but its also have some limitations which is we discuss in above the main limitation is N-gram does not able to work good with the machine translation."
      ],
      "metadata": {
        "id": "uAzHgoGcUaR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5 (10 points)\n",
        "\"What is the difference between an n-gram model and a neural language model in natural language processing, and which one is better for text generation?\""
      ],
      "metadata": {
        "id": "AyLvJdJ98eTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer here:**\n",
        "N-gram model : this is probability model that trained using the corpus text this is basically used for predicting the probability of the text based on the previous words or counting the words and predict the probability of next word.this i model is used for speech recognition and machine translation as well as predict the text.\n",
        "\n",
        "Neural language model : neural language model have the capacity to learn the distributed representations, which allow to mitigate the issue of curse of dimensionality because neural network have the ability to effect learn and represent complex pattern and relation between the data.\n",
        "\n",
        "A neural network language model is a language model based on Neural Networks , exploiting their ability to learn distributed representations to reduce the impact of the curse of dimensionality.\n",
        "\n",
        "N- gram models and neural language models both are used in natural language processing for text generation.However, they both has a different kind of approach for language models.\n",
        "\n",
        "N-gram models are a tyoe of statistical language model that will predict the probability of the next word in sequences based on condition n-1 words. For instance, Bigram model can predict the probability of the words based on the previous words in the sentence while n-gram is easy to implement, but its have a limitation context data and may not identify the longe range of the data.\n",
        "\n",
        "On the other hand, neural language models use artificial neural networks to  do the probability of words in sequences. This model use more layers of neurons to capture information and higher level features for language model.neural network trained in large amount of data and its capture complex relation between sequence of the words and making the better version of the generating natural sounding text.\n",
        "\n",
        "when talking about the text generation, neural language models are better than n-gram model. because they are generating more diverse and natural text,in contrast, neural languge model also more complex and comparitevily intense, so model may be slower to train and more difficult to implement then n-gram model.\n",
        "\n",
        "To all in all, while n-gram models are simple and easy to implement, neural language models are more powerful for creating natural sounding text.And on the other hand, the choice between two models its totally depends on the requirements of the nlp tasks."
      ],
      "metadata": {
        "id": "t1A0PYwI8fIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 2: Perplexity for the N-gram Language Model\n",
        "\n",
        "Using the test subset of the Reuters corpus, the below code determines the perplexity of a trigram language model.\n",
        "\n",
        "In n-gram language model, a typical statistic for assessing the effectiveness of language models is perplexity. It gauges how well a linguistic model can foretell a collection of test data. Less perplexity indicates improved test data prediction by the language model.\n",
        "\n",
        "The likelihood of the words in a test corpus and the probability of the same words predicted by the model are compared in the code below to get the perplexity of an n-gram model. All of the words in the test corpus are lowercased as part of the preprocessing process.\n",
        "\n",
        "The test corpus's overall word count is kept in the N variable. The probability of the words predicted by the model is computed using the log_prob variable. The loop iterates over the test corpus and computes the probability of the word given the context using the model for each context (a tuple of the two preceding words) and word in the corpus.\n",
        "\n",
        "The probability's logarithm is added to the log_prob variable if the probability is higher than 0. The logarithm of the probability of a new word is placed in its place if the probability is 0 (i.e., the model has never seen this context and word combination before). Using the equation perplexity = 2**(-log prob / N), the perplexity is finally calculated and reported."
      ],
      "metadata": {
        "id": "8F5w6obHycaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "test_corpus = reuters.sents(categories='interest')\n",
        "test_corpus = [word.lower() for sent in test_corpus for word in sent]\n",
        "\n",
        "# Calculate the perplexity\n",
        "N = len(test_corpus)\n",
        "log_prob = 0\n",
        "for i in range(2, N):\n",
        "    context = (test_corpus[i-2], test_corpus[i-1])\n",
        "    word = test_corpus[i]\n",
        "    if context in reutersmodel and reutersmodel[context][word] > 0:\n",
        "        log_prob += math.log(reutersmodel[context][word], 2)\n",
        "    else:\n",
        "        log_prob += math.log(1/N, 2)\n",
        "\n",
        "perplexity = 2**(-log_prob / N)\n",
        "print(\"Perplexity:\", perplexity)"
      ],
      "metadata": {
        "id": "PXue3l_E9YAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4a14eb-0970-490d-8a8e-aa2e9da2ee7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 81.44731624070452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, basically we are evaluating our language model and calculating the perplexity of the model.\n",
        "\n",
        "Perplexity : This is used for checking or evaluating the model accuracy that how well that model predicting the samples. this is basically evaluating the model accuracy.\n",
        "\n",
        "Here,for that we are performing some task as follows,\n",
        "import math : this is the library which is we are using for performing some mathematic operation for model.\n",
        "\n",
        "now after that we create a object for reuters text with parameter categories as a interest.after that we are converting text into the lower case.\n",
        "\n",
        "after all that we start begining of the calculating the perplexity. where first we provide the length of the test corpus. set constant value for log probabilty as 0. and the we are going to use for loop for range start with 2 up to the length of the test corpus text.here in for loop we are checking the probabilty based on the context words and mathematical operation where the condition is log proability 1/ N ,2 where N is the length of the test corpus text and 2 is the starting point of the loop or we can say starting word position of the loops. at the last we are calculating the perplexity where 2**(-log_prob/N) which means that 2 multiply with -log probabilty and devide by the length of the test corpus and at the last we are calculating the perplexity and print it. final perplexity with reuters corpus text is 81.44.The perplexity is lower than 100 its 81.44 it means the model is accurate and its predict the good reuslt or generate accurate text.\n",
        "\n",
        "In conclusion, we are evaluating the model and for that we are calculating the perplexity of the model using the perplexity calculation using the mathematics use.takling about the evaluation process of the model here we are checking that our model is generating the correct sample text or not using our basic language model.Finally, after all the calculation and after performing the operation we got perplexity is 81.44. It shows that model is very good in prediction with this model or we can say its more accurate because the perplexity count is under 100."
      ],
      "metadata": {
        "id": "0X3QxHw3JbCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6 (25 points)\n",
        "\n",
        "Using the Language model you created in tutorial 1, compute the perplexity for the same model. You can use the same dataset used in task 2. Provide the explanations for your method in detail."
      ],
      "metadata": {
        "id": "iaobD1e-zNDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "test_corpus = gutenberg.sents()\n",
        "\n",
        "test_corpus = [word.lower() for sent in test_corpus for word in sent]\n",
        "\n",
        "# Calculate the perplexity\n",
        "N = len(test_corpus)\n",
        "log_prob = 0\n",
        "for i in range(2, N):\n",
        "    context = (test_corpus[i-2], test_corpus[i-1])\n",
        "    word = test_corpus[i]\n",
        "    if context in gutenbergmodel and gutenbergmodel[context][word] > 0:\n",
        "        log_prob += math.log(gutenbergmodel[context][word], 2)\n",
        "    else:\n",
        "        log_prob += math.log(1/N, 2)\n",
        "\n",
        "perplexity = 2**(-log_prob / N)\n",
        "print(\"Perplexity:\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM8rme7jMWUz",
        "outputId": "537f2414-82b9-43ce-df78-5bea2272ed1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 295.4029482623658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer here:**\n",
        "Here, basically we are evaluating our language model and calculating the perplexity of the model.\n",
        "\n",
        "Perplexity : This is used for checking or evaluating the model accuracy that how well that model predicting the samples. this is basically evaluating the model accuracy.\n",
        "\n",
        "Here,for that we are performing some task as follows, import math : this is the library which is we are using for performing some mathematic operation for model.\n",
        "\n",
        "now after that we create a object for gutenberg text and object name as test_corpus.after that we are converting text into the lower case.\n",
        "\n",
        "after all that we start begining of the calculating the perplexity. where first we provide the length of the test corpus. set constant value for log probabilty as 0. and the we are going to use for loop for range start with 2 up to the length of the test corpus text.here in for loop we are checking the probabilty based on the context words and mathematical operation where the condition is log proability 1/ N ,2 where N is the length of the test corpus text and 2 is the starting point of the loop or we can say starting word position of the loops. at the last we are calculating the perplexity where 2**(-log_prob/N) which means that 2 multiply with -log probabilty and devide by the length of the test corpus and at the last we are calculating the perplexity and print it. final perplexity with gutenberg corpus text is 295.44.The perplexity is higher than 100 its 295.44. it means the model is not accurate and its not able to predict the good result or generate accurate text.\n",
        "\n",
        "In conclusion, we are evaluating the model and for that we are calculating the perplexity of the model using the perplexity calculation using the mathematics use.takling about the evaluation process of the model here we are checking that our model is generating the correct sample text or not using our basic language model.Finally, after all the calculation and after performing the operation we got perplexity is 295.44. It shows that model is not very good in predicting with this model or we can say its not accurate."
      ],
      "metadata": {
        "id": "Cxdy_FVjAL_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7 (5 points)\n",
        "Lower perplexity generally means better-performing language model. Do you agree with the statement? Explain why do you think so."
      ],
      "metadata": {
        "id": "L3Hvt_ol_Yl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer here:**\n",
        "\n",
        "Yes, I agree with the statement that lower perplexity generally means a better-performing language model.\n",
        "\n",
        "perplexity : this is calculating how the language model predicts the sequences of the words well and its give accurate result of the next prediction of the sequence of the words.talking in the simple way its phenomenon to see the result of language model prediction for next word for sequence of the words.If the perplexity is law then model is not work properly for the next word and its better its prediction.perplexity is majorly access because of the simplicity and effectiveness.\n",
        "\n",
        "Talking about the language model, the main aim is to get the lower perplexity of a model for given dataset. if the perplexity is law then its define that model is good with sequence of the next words.\n",
        "\n",
        "Lower perplexity refer to a language modal is capable to predict the future next words accurately.this will be pursued by reducing the uncertainty of the next word in the sequence of the words and that will lower the perplexity score.if the lower perplexity score then model is good in performing task. I agree with the statement that lower perplexity is shows that model is good with performance.\n",
        "\n",
        "A language model with a lower perplexity is to have better trained and represent of the language and underlying the patterns of the data.this type of model can identify the context and dependencies of two words in the sentence and finally after that results are more accurate for the next word.\n",
        "\n",
        "For instance, if we have two different language model such as Model A and Model B and we evaluate the models based on the given text. now, we are calculating the perplexity of both models, its indicates that Model A has a lower perplexity compared to Model B. its  implies that Model A is good at predicting the next words in a sentences and its has a good ability to understand the language underlying structure. so, focusing on the perplexity score we can conclude that Model A has a good performace than Model B.\n",
        "\n",
        "In summarize, lower perplexity is the symbol of better performance of the language model because they are good at predicting the next word in a sequyence of words and trained for better representation of the underlying language patterns."
      ],
      "metadata": {
        "id": "UEnOQhkW_buc"
      }
    }
  ]
}